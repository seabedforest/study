{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\n"
     ]
    }
   ],
   "source": [
    "import nltk.tokenize as tk\n",
    "doc = \"Are you curious about tokenization? \" \\\n",
    "      \"Let's see how it works! \" \\\n",
    "      \"We need to analyze a couple of sentences \" \\\n",
    "      \"with punctuations to see it in action.\"\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Are you curious about tokenization?',\n",
       " \"Let's see how it works!\",\n",
       " 'We need to analyze a couple of sentences with punctuations to see it in action.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#拆句子\n",
    "sents = tk.sent_tokenize(doc)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Are',\n",
       " 'you',\n",
       " 'curious',\n",
       " 'about',\n",
       " 'tokenization',\n",
       " '?',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'see',\n",
       " 'how',\n",
       " 'it',\n",
       " 'works',\n",
       " '!',\n",
       " 'We',\n",
       " 'need',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'sentences',\n",
       " 'with',\n",
       " 'punctuations',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'in',\n",
       " 'action',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=tk.word_tokenize(doc)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Are',\n",
       " 'you',\n",
       " 'curious',\n",
       " 'about',\n",
       " 'tokenization',\n",
       " '?',\n",
       " 'Let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'see',\n",
       " 'how',\n",
       " 'it',\n",
       " 'works',\n",
       " '!',\n",
       " 'We',\n",
       " 'need',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'sentences',\n",
       " 'with',\n",
       " 'punctuations',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'in',\n",
       " 'action',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1=tk.WordPunctTokenizer().tokenize(doc)\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as ft\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 9)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 9)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 9)\t1\n"
     ]
    }
   ],
   "source": [
    "#词袋模型\n",
    "doc=\"This hotel is very bad. The toilet in this hotel smells bad. The environment of this hotel is very good.\"\n",
    "sents=tk.sent_tokenize(doc)\n",
    "cv=ft.CountVectorizer()\n",
    "bow=cv.fit_transform(sents)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 1 0 0 0 1 0 1]\n",
      " [1 0 0 1 1 0 0 1 1 1 1 0]\n",
      " [0 1 1 1 0 1 1 0 1 1 0 1]]\n",
      "['bad', 'environment', 'good', 'hotel', 'in', 'is', 'of', 'smells', 'the', 'this', 'toilet', 'very']\n"
     ]
    }
   ],
   "source": [
    "print(bow.toarray())\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.488 0.    0.    0.379 0.    0.488 0.    0.    0.    0.379 0.    0.488]\n",
      " [0.345 0.    0.    0.268 0.454 0.    0.    0.454 0.345 0.268 0.454 0.   ]\n",
      " [0.    0.429 0.429 0.253 0.    0.326 0.429 0.    0.326 0.253 0.    0.326]]\n"
     ]
    }
   ],
   "source": [
    "#TFIDF\n",
    "tt=ft.TfidfTransformer()\n",
    "tfidf = tt.fit_transform(bow)\n",
    "print(np.round(tfidf.toarray(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
